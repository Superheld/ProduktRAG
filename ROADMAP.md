# ProduktRAG Implementation Roadmap

## Projekt-Neustart: Fundamentale Ãœberarbeitung ðŸ”„

**Status:** Kompletter Neuaufbau mit verbesserter Architektur
**Grund:** Erste Version hatte strukturelle Probleme bei DatenqualitÃ¤t und Chunking-Strategie
**Aktueller Fokus:** Saubere Normalisierung als Fundament fÃ¼r hochwertiges RAG-System

---

## Aktuelle Architektur-Entscheidungen

### Chunk-Strategie (NEU)
Drei-Chunk-Hierarchie fÃ¼r optimales Retrieval:

**1. Overview-Chunk (1x pro Produkt)**
- Zusammenfassung vom LLM generiert
- EnthÃ¤lt Kategorie + Hersteller in Metadata
- FÃ¼r breite Produkt-Queries

**2. Description-Chunks (mehrere pro Produkt)**
- LLM-bereinigte AbsÃ¤tze ohne Marketing/Firmengeschichte
- Semantisch kohÃ¤rente Einheiten
- FÃ¼r feature-spezifische Queries

**3. Spec-Group-Chunks (mehrere pro Produkt)**
- Gruppiert nach Kategorie (Abmessungen, Gewicht, etc.)
- Normalisierte Einheiten (cm, kg, l)
- Strukturierte Keys fÃ¼r Filtering
- FÃ¼r technische Multi-Kriterien-Suche

### LLM-Agenten-Pipeline (NEU)

**Summary-Agent:**
- Input: Komplettes Produkt
- Output: `{summary, category, manufacturer}`
- Generiert Overview-Chunk

**Description-Agent:**
- Input: Rohe Produktbeschreibung
- Output: Array von bereinigten AbsÃ¤tzen
- Entfernt Marketing, Herstellergeschichte, Redundanz

**Specs-Agent:**
- Input: Array von raw specs
- Output: Gruppierte, normalisierte Specs
- Deutsche Gruppennamen mit Einheit: `"Abmessungen-cm": {"AuÃŸenmaÃŸe_Breite": 67, ...}`
- Automatische Umrechnung: gâ†’kg, mlâ†’l, mmâ†’cm

### Warum diese Ã„nderungen?

**Problem der alten Version:**
- âŒ Leere/sehr kurze Chunks (z.B. "## Kirsch")
- âŒ Zu granulare Specs (jede einzeln â†’ schlechtes Multi-Kriterien-Retrieval)
- âŒ Marketing-Noise in Descriptions
- âŒ Inkonsistente Einheiten
- âŒ Keine hierarchische Struktur

**Vorteile der neuen Architektur:**
- âœ… Overview fÃ¼r schnellen Produkt-Ãœberblick
- âœ… Saubere, fokussierte Description-Chunks
- âœ… Gruppierte Specs fÃ¼r besseres Matching
- âœ… Normalisierte Daten (cm, kg, l statt mm/cm/m, g/kg, ml/l)
- âœ… Strukturierte Metadata fÃ¼r Hybrid-Search

---

## Current Status

### âœ… COMPLETED: Infrastructure & Tooling
- Agent-Prompts definiert (Summary, Description, Specs)
- LLM-Pipeline mit Mistral implementiert
- Pandas-basierte Datenverarbeitung
- JSONL-Output-Format fÃ¼r Chunks

### ðŸ”„ IN PROGRESS: Phase 1 - Data Normalization & Chunking

**Aktueller Stand:**
- âœ… Raw-Data vorhanden (152 Produkte)
- âœ… Agent-Prompts finalisiert und getestet
- âœ… Code-Struktur fÃ¼r 3-Agenten-Pipeline
- ðŸ”„ Full-Run durch alle 152 Produkte (in Arbeit)
- ðŸ“‹ QualitÃ¤tskontrolle der Agent-Outputs

**Output-Struktur:**
```
1-normalisation/
â”œâ”€â”€ overview_chunks.jsonl      # 152 Chunks (1 pro Produkt)
â”œâ”€â”€ description_chunks.jsonl   # ~600-800 Chunks (mehrere pro Produkt)
â””â”€â”€ specs_chunks.jsonl         # ~1200-1500 Chunks (9 Kategorien x ~150 Produkte)
```

**Erwartete Chunk-Anzahl:** ~2000-2500 (vs. alte 4618 mit vielen schlechten)

### ðŸ“‹ TODO: Phase 2 - Embedding Generation

**Ziel:** Chunks in Vektoren umwandeln mit solidem deutschen Model

**Model-Auswahl:**
- **Start:** `intfloat/multilingual-e5-large`
  - Robustes Multilingual-Model mit sehr guter Performance
  - Gut fÃ¼r deutsche technische Fachsprache
  - 1024 Dimensionen
  - Schnell genug fÃ¼r ~2500 Chunks

**Tasks:**
1. Model laden und vorbereiten
2. Batch-Processing aller normalisierten Chunks (summs + descs + specs)
3. Normalisierung der Embeddings (L2-Normalisierung)
4. Speicherung als `.npy` fÃ¼r schnellen Load
5. QualitÃ¤tschecks (keine NaNs, korrekte Dimensionen)

**Expected Output:**
```
2-embedding/
â”œâ”€â”€ embeddings_e5_large.npy          # ~20-30MB
â”œâ”€â”€ chunks_combined.jsonl            # Alle Chunks in einem File
â””â”€â”€ embedding_metadata.json          # Model-Info, Timestamp, Dimensionen
```

**Dimensions:** [~2500, 1024]

### ðŸ“‹ TODO: Phase 3 - Vector Database Integration

**Technology:** ChromaDB (einfach, lokal, perfekt fÃ¼r Lernen)

**Setup:**
1. ChromaDB installieren
2. Collection erstellen mit e5-large-Embeddings
3. Alle Chunks mit Metadata laden
4. Index aufbauen

**Schema:**
```python
chunk = {
    "document": "Abmessungen (cm) - AuÃŸenmaÃŸe Breite: 67, Tiefe: 72, HÃ¶he: 132",
    "embedding": [...],  # 1024-dim vector
    "metadata": {
        "chunk_type": "overview|description|specs",
        "product_id": "LABO-288",
        "product_title": "Kirsch LABO-288 PRO-ACTIVE",
        "product_url": "https://...",
        "product_category": "LaborkÃ¼hlschrank",  # Nur bei overview
        "product_manufacturer": "Kirsch",         # Nur bei overview
        "spec_category": "Abmessungen-cm",        # Nur bei specs
        "specs": {"AuÃŸenmaÃŸe_Breite": 67, ...}   # Nur bei specs
    }
}
```

**Output:**
```
3-indexing/
â”œâ”€â”€ chroma_db/                       # ChromaDB Persistenz
â”œâ”€â”€ setup_index.ipynb                # Setup-Code
â””â”€â”€ test_queries.ipynb               # Erste Query-Tests
```

### ðŸ“‹ TODO: Phase 4 - Retrieval Evaluation

**Ziel:** Testen ob Chunking-Strategie und Embeddings funktionieren

**Test-Queries:**
```python
test_queries = {
    'overview': "Welche LaborkÃ¼hlschrÃ¤nke von Liebherr gibt es?",
    'technical_specs': "KÃ¼hlschrank mit 280L und max. 150cm HÃ¶he",
    'features': "Wie funktioniert die TemperaturÃ¼berwachung?",
    'hybrid': "Energieeffizienter MedikamentenkÃ¼hlschrank mit SmartMonitoring"
}
```

**Evaluation-Metriken:**
- **Precision@K** - Wie viele der Top-K sind relevant?
- **Recall@K** - Wie viele relevante Docs wurden gefunden?
- **MRR** - Position des ersten relevanten Dokuments
- **NDCG** - Ranking-QualitÃ¤t

**Success Criteria:**
- Precision@3: >80%
- Recall@5: >70%
- MRR: >0.8
- Overview-Chunks ranken hoch bei Produkt-Queries
- Spec-Chunks ranken hoch bei technischen Multi-Kriterien-Queries
- Description-Chunks ranken hoch bei Feature-Queries

**Falls schlecht:** â†’ Phase 5 (Model Evaluation)

### ðŸ“‹ TODO: Phase 5 - Model Evaluation (Optional)

**Wann:** Nur falls Phase 4 (Retrieval Evaluation) schlechte Ergebnisse zeigt

**Ziel:** Besseres Embedding-Model fÃ¼r deutsche medizinische Fachsprache finden

**Models to Test:**
- `deepset/gbert-large` - Deutsche BERT-Variante
- `GerMedBERT/medbert-512` - Medizin-spezifisch
- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
- Weitere nach Bedarf

**Evaluation-Methoden:**
1. **Semantic Similarity Tests** - Fachbegriffs-Paare (z.B. "MedikamentenkÃ¼hlschrank" â†” "Pharmazeutische Lagerung")
2. **Retrieval-Tests** - Dieselben Test-Queries aus Phase 4
3. **Performance-Metrics** - Embedding-Speed, Memory-Usage

**Success Criteria:**
- Bessere Metrics als e5-large in Phase 4
- Gute Differenzierung zwischen technischen Begriffen
- Robustheit gegenÃ¼ber Synonymen
- Performant genug fÃ¼r Produktion (<2s fÃ¼r 2500 Chunks)

**Output:**
- Falls besseres Model gefunden â†’ Phase 2 & 3 wiederholen
- Sonst â†’ Weiter zu Phase 6

### ðŸ“‹ TODO: Phase 6 - Production RAG Pipeline

**Components:**
1. Query Interface (FastAPI oder Streamlit)
2. Query â†’ Embedding
3. Vector Search (ChromaDB)
4. Context Assembly (Top-K Chunks + Metadata)
5. LLM Integration (Claude/GPT/Mistral)
6. Response Generation
7. Caching + Logging

**Advanced Features (spÃ¤ter):**
- Hybrid Search: Semantic + Structured Filtering
- Multi-Stage Retrieval: Overview â†’ Details
- Metadata-basierte Pre-/Post-Filtering
- Re-Ranking

---

## Neue Chunk-Schema Dokumentation

### Overview-Chunk
```json
{
  "document": "Der Kirsch LABO-288 ist ein LaborkÃ¼hlschrank mit 280L Volumen...",
  "metadata": {
    "chunk_type": "overview",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://...",
    "product_category": "LaborkÃ¼hlschrank",
    "product_manufacturer": "Kirsch"
  }
}
```

### Description-Chunk
```json
{
  "document": "Die elektronische Temperatursteuerung regelt die Temperatur...",
  "metadata": {
    "chunk_type": "description",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://..."
  }
}
```

### Specs-Chunk
```json
{
  "document": "Abmessungen (cm) - AuÃŸenmaÃŸe Breite: 67, Tiefe: 72, HÃ¶he: 132",
  "metadata": {
    "chunk_type": "specs",
    "spec_category": "Abmessungen-cm",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://...",
    "specs": {
      "AuÃŸenmaÃŸe_Breite": 67,
      "AuÃŸenmaÃŸe_Tiefe": 72,
      "AuÃŸenmaÃŸe_HÃ¶he": 132
    }
  }
}
```

---

## Updated File Structure

```
ProduktRAG/
â”œâ”€â”€ _docs/                              # ðŸ“š Documentation
â”‚   â”œâ”€â”€ CHUNKING-DEEP-DIVE.md          # âœ… Umfassender Chunking-Guide
â”‚   â””â”€â”€ EMBEDDING-STRATEGIES.md        # âœ… Embedding-Best-Practices
â”œâ”€â”€ 1-chunking/                        # ðŸ”„ IN PROGRESS - Normalisierung + Chunking
â”‚   â”œâ”€â”€ agent_description.md           # âœ… Description-Agent Prompt
â”‚   â”œâ”€â”€ agent_specs.md                 # âœ… Specs-Agent Prompt
â”‚   â”œâ”€â”€ agent_summary.md               # âœ… Summary-Agent Prompt
â”‚   â”œâ”€â”€ 1-cleanup.ipynb                # ðŸ”„ LLM-Pipeline Implementation
â”‚   â”œâ”€â”€ products_raw.json              # âœ… Raw data (152 Produkte)
â”‚   â”œâ”€â”€ summs_chunks.jsonl             # ðŸ“‹ TODO - Overview chunks
â”‚   â”œâ”€â”€ descs_chunks.jsonl             # ðŸ“‹ TODO - Description chunks
â”‚   â””â”€â”€ specs_chunks.jsonl             # ðŸ“‹ TODO - Specs chunks
â”œâ”€â”€ 2-embedding/                       # ðŸ“‹ TODO - Embedding generation
â”‚   â”œâ”€â”€ generate_embeddings.ipynb      # Embedding-Pipeline
â”‚   â”œâ”€â”€ embeddings_e5_large.npy        # Vector-Ausgabe
â”‚   â”œâ”€â”€ chunks_combined.jsonl          # Alle Chunks kombiniert
â”‚   â””â”€â”€ embedding_metadata.json        # Model-Info
â”œâ”€â”€ 3-indexing/                        # ðŸ“‹ TODO - ChromaDB setup
â”‚   â”œâ”€â”€ chroma_db/                     # ChromaDB Persistenz
â”‚   â”œâ”€â”€ setup_index.ipynb              # Index erstellen
â”‚   â””â”€â”€ test_queries.ipynb             # Erste Tests
â”œâ”€â”€ 4-eval-retrieval/                  # ðŸ“‹ TODO - Retrieval evaluation
â”‚   â”œâ”€â”€ eval_retrieval.ipynb           # Metrics berechnen
â”‚   â””â”€â”€ test_queries.json              # Test-Query-Set
â”œâ”€â”€ 5-eval-model/                      # ðŸ“‹ TODO OPTIONAL - Model comparison
â”‚   â”œâ”€â”€ compare_models.ipynb           # Model-Vergleich
â”‚   â””â”€â”€ results.json                   # Evaluation-Results
â”œâ”€â”€ 6-production/                      # ðŸ“‹ TODO - Production RAG
â”‚   â”œâ”€â”€ app.py                         # FastAPI/Streamlit
â”‚   â”œâ”€â”€ rag_pipeline.py                # RAG-Logik
â”‚   â””â”€â”€ config.yaml                    # Konfiguration
â”œâ”€â”€ ROADMAP.md                         # âœ… This file
â””â”€â”€ requirements.txt                   # âœ… Dependencies
```

---

## Lessons Learned (aus Version 1)

### Was schiefging:
- **Chunking zu frÃ¼h:** Ohne Datenbereinigung â†’ viele schlechte Chunks
- **Kein LLM fÃ¼r Normalisierung:** Manuelle Regex-Spielchen ineffektiv
- **Zu granulare Specs:** Jede Spec einzeln â†’ schlechtes Multi-Kriterien-Matching
- **Keine Hierarchie:** Alle Chunks gleich behandelt

### Was wir jetzt besser machen:
- **LLM-First Approach:** Lass das LLM die Arbeit machen
- **Hierarchische Chunks:** Overview, Details, Specs fÃ¼r verschiedene Query-Typen
- **Strukturierte Normalisierung:** Einheiten vereinheitlicht, gruppiert
- **Quality First:** Lieber 2500 gute Chunks als 4618 mit Noise

### NÃ¤chste Schritte nach Chunking:
1. **Embedding** - Mit multilingual-e5-large starten
2. **Indexing** - ChromaDB aufsetzen
3. **Retrieval Evaluation** - Testen ob's funktioniert
4. **Model Evaluation** - Nur falls nÃ¶tig, andere Models probieren
5. **Production** - RAG-Pipeline bauen

---

## Performance Targets

### Data Quality (Phase 1):
- âœ… Alle Chunks >10 Zeichen
- âœ… Keine Marketing-Floskeln in Descriptions
- âœ… Konsistente Einheiten (cm, kg, l)
- âœ… Strukturierte Gruppierung

### Embedding Performance (Phase 2):
- Embedding-Zeit fÃ¼r 2500 Chunks: <2min
- Embedding-Dimensionen: 1024
- Keine NaNs oder Inf-Werte

### Retrieval Quality (Phase 4):
- Precision@3: >80%
- Recall@5: >70%
- MRR: >0.8
- NDCG: >0.75

### Production Performance (Phase 6):
- Query Latency: <3s (Embedding + Retrieval + Generation)
- Memory Usage: <4GB
- Scalability: 100+ concurrent queries

---

## Documentation

Siehe `_docs/` Ordner fÃ¼r:
- **CHUNKING-DEEP-DIVE.md** - Warum, Was, Wie? (Universell einsetzbar)
- **RAG-EMBEDDING-STRATEGIES.md** - Best Practices fÃ¼r Embeddings

---

*Last updated: 2025-10-03*
*Status: Phase 1 (Chunking) in Arbeit - 60% Complete*

---

## Quick Reference: Pipeline-Ãœberblick

```
Phase 1: Chunking âœ… (in Arbeit)
   â†“
Phase 2: Embedding (multilingual-e5-large)
   â†“
Phase 3: Indexing (ChromaDB)
   â†“
Phase 4: Retrieval Evaluation (testen!)
   â†“
Phase 5: Model Evaluation (nur falls Phase 4 schlecht)
   â†“
Phase 6: Production RAG
```
