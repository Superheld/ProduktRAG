# ProduktRAG Implementation Roadmap

## Projekt-Neustart: Fundamentale Ãœberarbeitung ðŸ”„

**Status:** Kompletter Neuaufbau mit verbesserter Architektur
**Grund:** Erste Version hatte strukturelle Probleme bei DatenqualitÃ¤t und Chunking-Strategie
**Aktueller Fokus:** Saubere Normalisierung als Fundament fÃ¼r hochwertiges RAG-System

## Current Status

### âœ… COMPLETED: Infrastructure & Tooling
- Agent-Prompts definiert (Summary, Description, Specs)
- LLM-Pipeline mit Mistral implementiert
- Pandas-basierte Datenverarbeitung
- JSONL-Output-Format fÃ¼r Chunks

### âœ… COMPLETED: Phase 1 - Data Normalization & Chunking

**Erledigt:**
- âœ… Raw-Data bereinigt (leere Produkte entfernt)
- âœ… Drei LLM-Agenten implementiert und getestet:
  - **summs_agent.md**: Generiert Zusammenfassung + extrahiert Category/Manufacturer
  - **descs_agent.md**: Bereinigt Produktbeschreibungen von Marketing-Floskeln
  - **specs_agent.md**: Normalisiert technische Specs mit zwei Input-Formaten (Array + Text-Fallback)
- âœ… Notebook refactored:
  - `chunk_id` fÃ¼r alle Chunks (Format: `{product_id}_{type}_{index}`)
  - `base_metadata` mit spread operator (`**`) fÃ¼r DRY-Code
  - Fallback-Handling fÃ¼r Produkte ohne Specs
  - `enumerate()` fÃ¼r Index-Tracking
- âœ… Schema-Verbesserungen:
  - specs_agent verarbeitet jetzt Array-Input UND Text-Input konsistent
  - Alle 12 Spec-Gruppen immer im Output (auch wenn leer)
  - VerstÃ¤rkte Prompt-Regeln fÃ¼r konsistentes Output-Schema

**Output-Struktur:**
```
1-normalisation/
â”œâ”€â”€ summs_chunks.jsonl      # Summary chunks (1 pro Produkt)
â”œâ”€â”€ descs_chunks.jsonl      # Description chunks (mehrere pro Produkt)
â””â”€â”€ specs_chunks.jsonl      # Specs chunks (gruppiert nach Kategorien)
```

**Key Learnings:**
- LLM benÃ¶tigt sehr explizite Schema-Definitionen (Beispiele > abstrakte Regeln)
- Fallback-Handling wichtig fÃ¼r fehlende/inkonsistente Daten
- Metadata-Struktur sollte frÃ¼h finalisiert werden (chunk_id, base_metadata)

### âœ… COMPLETED: Phase 2 - Embedding Generation

**Ziel:** Chunks in Vektoren umwandeln mit solidem deutschen Model âœ…

**Model-Wahl:**
- **Entscheidung:** `deepset/gbert-large` (deutsch-only)
- 1024 Dimensionen, normalisierte Embeddings
- Batch-Processing mit `sentence-transformers`

**Was erledigt wurde:**
1. âœ… Alle 3 Chunk-Types kombiniert (summs, descs, specs)
2. âœ… Datenbereinigung:
   - Whitespace entfernt (`.str.strip()`)
   - Leere Dokumente gefiltert (`len > 10`)
   - Duplikate entfernt (basierend auf `document`)
3. âœ… Batch-Embedding:
   - 1800 Chunks in 17:40 Min (batch_size=16)
   - L2-Normalisierung aktiviert
   - CPU-basiert (Intel GPU Setup zu komplex fÃ¼r jetzt)
4. âœ… Validierung implementiert:
   - LÃ¤ngen-Check (Embeddings â†” Chunks)
   - 1% Stichprobe mit Norm-Check (alle ~1.0000 âœ…)
   - Re-Encode Similarity-Test (alle >0.9999 âœ…)
5. âœ… Speicherung:
   - `embeddings_gbert.npy` - Binary numpy array (1800, 1024)
   - `chunks_metadata.jsonl` - Alle Chunks mit Metadata
   - Index-Mapping: `embeddings[i]` â†” `chunks_metadata.iloc[i]`

**Output-Struktur:**
```
2-embedding/
â”œâ”€â”€ embeddings_gbert.npy          # Vector array (1800, 1024)
â”œâ”€â”€ chunks_metadata.jsonl         # Alle Chunks mit chunk_id
â””â”€â”€ 1-embeddings.ipynb            # Notebook mit Pipeline + Validation
```

**Key Learnings:**
- Index-basiertes Mapping (statt SQL-Joins) ist simpel und effizient
- Batch-Processing ist 10-50x schneller als einzelne Embeddings
- Validierung durch Re-Encoding gibt 100% Sicherheit
- `.npy` fÃ¼r Embeddings + `.jsonl` fÃ¼r Metadata ist Best Practice
- Norm â‰ˆ 1.0 bei normalisierten Embeddings bestÃ¤tigt Korrektheit

**Achievements:**
- ðŸŽ¯ 1800 saubere, validierte Embeddings
- ðŸŽ¯ Robuste Pipeline mit Fehler-Checks
- ðŸŽ¯ Reproduzierbar und gut dokumentiert
- ðŸŽ¯ Bereit fÃ¼r Phase 3 (Retrieval)

### âœ… COMPLETED: Phase 3 - Indexing with ChromaDB

**Entscheidung:** ChromaDB (Production-ready, Metadata-Filtering, einfache Integration)

**Was erledigt wurde:**
1. âœ… ChromaDB installiert und PersistentClient aufgesetzt
2. âœ… Collection "prdukt_chunks" erstellt mit GBERT-Embeddings
3. âœ… 1800 Chunks mit Metadata indexiert
4. âœ… Datenbank-Kontrolle im Notebook implementiert

**Output:**
```
3-indexing/
â”œâ”€â”€ chroma_db/
â”‚   â””â”€â”€ chroma.sqlite3               # ChromaDB Persistenz
â””â”€â”€ 01-indexing.ipynb                # Indexing + Kontrolle
```

**Key Learnings:**
- IDs mÃ¼ssen als Strings gespeichert werden (ChromaDB-Requirement)
- Index-basiertes Mapping bleibt konsistent mit Phase 2
- ChromaDB lÃ¤dt Embeddings direkt als Listen (`.tolist()`)
- SQLite-basierte Persistenz ermÃ¶glicht einfaches Debugging

---

### ðŸŽ¯ NEXT: Phase 4 - Retrieval Evaluation

**Ziel:** Testen ob Chunking-Strategie und Embeddings funktionieren

**Test-Queries:**
```python
test_queries = {
    'overview': "Welche LaborkÃ¼hlschrÃ¤nke von Liebherr gibt es?",
    'technical_specs': "KÃ¼hlschrank mit 280L und max. 150cm HÃ¶he",
    'features': "Wie funktioniert die TemperaturÃ¼berwachung?",
    'hybrid': "Energieeffizienter MedikamentenkÃ¼hlschrank mit SmartMonitoring"
}
```

**Evaluation-Metriken:**
- **Precision@K** - Wie viele der Top-K sind relevant?
- **Recall@K** - Wie viele relevante Docs wurden gefunden?
- **MRR** - Position des ersten relevanten Dokuments
- **NDCG** - Ranking-QualitÃ¤t

**Success Criteria:**
- Precision@3: >80%
- Recall@5: >70%
- MRR: >0.8
- Overview-Chunks ranken hoch bei Produkt-Queries
- Spec-Chunks ranken hoch bei technischen Multi-Kriterien-Queries
- Description-Chunks ranken hoch bei Feature-Queries

**Falls schlecht:** â†’ Phase 5 (Model Evaluation)

### ðŸ“‹ TODO: Phase 5 - Model Evaluation (Optional)

**Wann:** Nur falls Phase 4 (Retrieval Evaluation) schlechte Ergebnisse zeigt

**Ziel:** Besseres Embedding-Model fÃ¼r deutsche medizinische Fachsprache finden

**Models to Test:**
- `deepset/gbert-large` - Deutsche BERT-Variante
- `GerMedBERT/medbert-512` - Medizin-spezifisch
- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
- Weitere nach Bedarf

**Evaluation-Methoden:**
1. **Semantic Similarity Tests** - Fachbegriffs-Paare (z.B. "MedikamentenkÃ¼hlschrank" â†” "Pharmazeutische Lagerung")
2. **Retrieval-Tests** - Dieselben Test-Queries aus Phase 4
3. **Performance-Metrics** - Embedding-Speed, Memory-Usage

**Success Criteria:**
- Bessere Metrics als e5-large in Phase 4
- Gute Differenzierung zwischen technischen Begriffen
- Robustheit gegenÃ¼ber Synonymen
- Performant genug fÃ¼r Produktion (<2s fÃ¼r 2500 Chunks)

**Output:**
- Falls besseres Model gefunden â†’ Phase 2 & 3 wiederholen
- Sonst â†’ Weiter zu Phase 6

### ðŸ“‹ TODO: Phase 6 - Production RAG Pipeline

**Components:**
1. Query Interface (FastAPI oder Streamlit)
2. Query â†’ Embedding
3. Vector Search (ChromaDB)
4. Context Assembly (Top-K Chunks + Metadata)
5. LLM Integration (Claude/GPT/Mistral)
6. Response Generation
7. Caching + Logging

**Advanced Features (spÃ¤ter):**
- Hybrid Search: Semantic + Structured Filtering
- Multi-Stage Retrieval: Overview â†’ Details
- Metadata-basierte Pre-/Post-Filtering
- Re-Ranking

---

## Neue Chunk-Schema Dokumentation

### Overview-Chunk
```json
{
  "document": "Der Kirsch LABO-288 ist ein LaborkÃ¼hlschrank mit 280L Volumen...",
  "metadata": {
    "chunk_type": "overview",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://...",
    "product_category": "LaborkÃ¼hlschrank",
    "product_manufacturer": "Kirsch"
  }
}
```

### Description-Chunk
```json
{
  "document": "Die elektronische Temperatursteuerung regelt die Temperatur...",
  "metadata": {
    "chunk_type": "description",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://..."
  }
}
```

### Specs-Chunk
```json
{
  "document": "Abmessungen (cm) - AuÃŸenmaÃŸe Breite: 67, Tiefe: 72, HÃ¶he: 132",
  "metadata": {
    "chunk_type": "specs",
    "spec_category": "Abmessungen-cm",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://...",
    "specs": {
      "AuÃŸenmaÃŸe_Breite": 67,
      "AuÃŸenmaÃŸe_Tiefe": 72,
      "AuÃŸenmaÃŸe_HÃ¶he": 132
    }
  }
}
```

---

## Updated File Structure

```
ProduktRAG/
â”œâ”€â”€ _docs/                              # ðŸ“š Documentation
â”‚   â”œâ”€â”€ CHUNKING-DEEP-DIVE.md          # âœ… Umfassender Chunking-Guide
â”‚   â””â”€â”€ EMBEDDING-STRATEGIES.md        # âœ… Embedding-Best-Practices
â”œâ”€â”€ 1-chunking/                        # ðŸ”„ IN PROGRESS - Normalisierung + Chunking
â”‚   â”œâ”€â”€ agent_description.md           # âœ… Description-Agent Prompt
â”‚   â”œâ”€â”€ agent_specs.md                 # âœ… Specs-Agent Prompt
â”‚   â”œâ”€â”€ agent_summary.md               # âœ… Summary-Agent Prompt
â”‚   â”œâ”€â”€ 1-cleanup.ipynb                # ðŸ”„ LLM-Pipeline Implementation
â”‚   â”œâ”€â”€ products_raw.json              # âœ… Raw data (152 Produkte)
â”‚   â”œâ”€â”€ summs_chunks.jsonl             # ðŸ“‹ TODO - Overview chunks
â”‚   â”œâ”€â”€ descs_chunks.jsonl             # ðŸ“‹ TODO - Description chunks
â”‚   â””â”€â”€ specs_chunks.jsonl             # ðŸ“‹ TODO - Specs chunks
â”œâ”€â”€ 2-embedding/                       # ðŸ“‹ TODO - Embedding generation
â”‚   â”œâ”€â”€ generate_embeddings.ipynb      # Embedding-Pipeline
â”‚   â”œâ”€â”€ embeddings_e5_large.npy        # Vector-Ausgabe
â”‚   â”œâ”€â”€ chunks_combined.jsonl          # Alle Chunks kombiniert
â”‚   â””â”€â”€ embedding_metadata.json        # Model-Info
â”œâ”€â”€ 3-indexing/                        # ðŸ“‹ TODO - ChromaDB setup
â”‚   â”œâ”€â”€ chroma_db/                     # ChromaDB Persistenz
â”‚   â”œâ”€â”€ setup_index.ipynb              # Index erstellen
â”‚   â””â”€â”€ test_queries.ipynb             # Erste Tests
â”œâ”€â”€ 4-eval-retrieval/                  # ðŸ“‹ TODO - Retrieval evaluation
â”‚   â”œâ”€â”€ eval_retrieval.ipynb           # Metrics berechnen
â”‚   â””â”€â”€ test_queries.json              # Test-Query-Set
â”œâ”€â”€ 5-eval-model/                      # ðŸ“‹ TODO OPTIONAL - Model comparison
â”‚   â”œâ”€â”€ compare_models.ipynb           # Model-Vergleich
â”‚   â””â”€â”€ results.json                   # Evaluation-Results
â”œâ”€â”€ 6-production/                      # ðŸ“‹ TODO - Production RAG
â”‚   â”œâ”€â”€ app.py                         # FastAPI/Streamlit
â”‚   â”œâ”€â”€ rag_pipeline.py                # RAG-Logik
â”‚   â””â”€â”€ config.yaml                    # Konfiguration
â”œâ”€â”€ ROADMAP.md                         # âœ… This file
â””â”€â”€ requirements.txt                   # âœ… Dependencies
```

---

## Lessons Learned (aus Version 1)

### Was schiefging:
- **Chunking zu frÃ¼h:** Ohne Datenbereinigung â†’ viele schlechte Chunks
- **Kein LLM fÃ¼r Normalisierung:** Manuelle Regex-Spielchen ineffektiv
- **Zu granulare Specs:** Jede Spec einzeln â†’ schlechtes Multi-Kriterien-Matching
- **Keine Hierarchie:** Alle Chunks gleich behandelt

### Was wir jetzt besser machen:
- **LLM-First Approach:** Lass das LLM die Arbeit machen
- **Hierarchische Chunks:** Overview, Details, Specs fÃ¼r verschiedene Query-Typen
- **Strukturierte Normalisierung:** Einheiten vereinheitlicht, gruppiert
- **Quality First:** Lieber 2500 gute Chunks als 4618 mit Noise

### NÃ¤chste Schritte nach Chunking:
1. **Embedding** - Mit multilingual-e5-large starten
2. **Indexing** - ChromaDB aufsetzen
3. **Retrieval Evaluation** - Testen ob's funktioniert
4. **Model Evaluation** - Nur falls nÃ¶tig, andere Models probieren
5. **Production** - RAG-Pipeline bauen

---

## Performance Targets

### Data Quality (Phase 1):
- âœ… Alle Chunks >10 Zeichen
- âœ… Keine Marketing-Floskeln in Descriptions
- âœ… Konsistente Einheiten (cm, kg, l)
- âœ… Strukturierte Gruppierung

### Embedding Performance (Phase 2):
- Embedding-Zeit fÃ¼r 2500 Chunks: <2min
- Embedding-Dimensionen: 1024
- Keine NaNs oder Inf-Werte

### Retrieval Quality (Phase 4):
- Precision@3: >80%
- Recall@5: >70%
- MRR: >0.8
- NDCG: >0.75

### Production Performance (Phase 6):
- Query Latency: <3s (Embedding + Retrieval + Generation)
- Memory Usage: <4GB
- Scalability: 100+ concurrent queries

---

## Documentation

Siehe `_docs/` Ordner fÃ¼r:
- **CHUNKING-DEEP-DIVE.md** - Warum, Was, Wie? (Universell einsetzbar)
- **RAG-EMBEDDING-STRATEGIES.md** - Best Practices fÃ¼r Embeddings

---

*Last updated: 2025-10-05*
*Status: Phase 1 (Chunking) âœ… | Phase 2 (Embedding) âœ… | Phase 3 (Indexing) âœ… | Phase 4 (Retrieval Eval) ðŸŽ¯ Next*

---

## Quick Reference: Pipeline-Ãœberblick

```
Phase 1: Chunking âœ… DONE
   â†“
Phase 2: Embedding âœ… DONE (gbert-large, 1800 chunks)
   â†“
Phase 3: Indexing âœ… DONE (ChromaDB, 1800 chunks)
   â†“
Phase 4: Retrieval Evaluation ðŸŽ¯ NEXT (testen!)
   â†“
Phase 5: Model Evaluation (nur falls Phase 4 schlecht)
   â†“
Phase 6: Production RAG
```

### Phase 1 Summary (Completed)
- 3 LLM-Agenten fÃ¼r Normalisierung (summs, descs, specs)
- Chunk-Schema mit chunk_id und strukturierter Metadata
- Specs-Agent mit Dual-Input-Support (Array + Text)
- Bereinigtes Dataset ohne leere Produkte
- Output: 3x JSONL-Files (summs, descs, specs)

### Phase 2 Summary (Completed)
- GBERT-large Embeddings (1024-dim, normalisiert)
- 1800 Chunks in 17:40 Min batch-processed
- Validierung: Norm-Check + Re-Encode Similarity
- Index-basiertes Mapping zwischen Embeddings & Metadata
- Output: embeddings_gbert.npy + chunks_metadata.jsonl

### Phase 3 Summary (Completed)
- ChromaDB PersistentClient mit SQLite-Backend
- Collection "prdukt_chunks" mit 1800 indexierten Chunks
- Metadata inkl. chunk_type, product_id, etc.
- Output: chroma_db/chroma.sqlite3
