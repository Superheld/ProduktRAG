# ProduktRAG Implementation Roadmap

## Projekt-Neustart: Fundamentale Ãœberarbeitung ðŸ”„

**Status:** Kompletter Neuaufbau mit verbesserter Architektur
**Grund:** Erste Version hatte strukturelle Probleme bei DatenqualitÃ¤t und Chunking-Strategie
**Aktueller Fokus:** Saubere Normalisierung als Fundament fÃ¼r hochwertiges RAG-System

## Current Status

### âœ… COMPLETED: Infrastructure & Tooling
- Agent-Prompts definiert (Summary, Description, Specs)
- LLM-Pipeline mit Mistral implementiert
- Pandas-basierte Datenverarbeitung
- JSONL-Output-Format fÃ¼r Chunks

### âœ… COMPLETED: Phase 1 - Data Normalization & Chunking

**Erledigt:**
- âœ… Raw-Data bereinigt (leere Produkte entfernt)
- âœ… Drei LLM-Agenten implementiert und getestet:
  - **summs_agent.md**: Generiert Zusammenfassung + extrahiert Category/Manufacturer
  - **descs_agent.md**: Bereinigt Produktbeschreibungen von Marketing-Floskeln
  - **specs_agent.md**: Normalisiert technische Specs mit zwei Input-Formaten (Array + Text-Fallback)
- âœ… Notebook refactored:
  - `chunk_id` fÃ¼r alle Chunks (Format: `{product_id}_{type}_{index}`)
  - `base_metadata` mit spread operator (`**`) fÃ¼r DRY-Code
  - Fallback-Handling fÃ¼r Produkte ohne Specs
  - `enumerate()` fÃ¼r Index-Tracking
- âœ… Schema-Verbesserungen:
  - specs_agent verarbeitet jetzt Array-Input UND Text-Input konsistent
  - Alle 12 Spec-Gruppen immer im Output (auch wenn leer)
  - VerstÃ¤rkte Prompt-Regeln fÃ¼r konsistentes Output-Schema

**Output-Struktur:**
```
1-normalisation/
â”œâ”€â”€ summs_chunks.jsonl      # Summary chunks (1 pro Produkt)
â”œâ”€â”€ descs_chunks.jsonl      # Description chunks (mehrere pro Produkt)
â””â”€â”€ specs_chunks.jsonl      # Specs chunks (gruppiert nach Kategorien)
```

**Key Learnings:**
- LLM benÃ¶tigt sehr explizite Schema-Definitionen (Beispiele > abstrakte Regeln)
- Fallback-Handling wichtig fÃ¼r fehlende/inkonsistente Daten
- Metadata-Struktur sollte frÃ¼h finalisiert werden (chunk_id, base_metadata)

### ðŸŽ¯ NEXT: Phase 2 - Embedding Generation

**Ziel:** Chunks in Vektoren umwandeln mit solidem deutschen Model

**Model-Auswahl:**
- **Start:** `intfloat/multilingual-e5-large`
  - Robustes Multilingual-Model mit sehr guter Performance
  - Gut fÃ¼r deutsche technische Fachsprache
  - 1024 Dimensionen
  - Schnell genug fÃ¼r alle Chunks

**Tasks:**
1. âœ… Chunks aus Phase 1 vorhanden (summs_chunks.jsonl, descs_chunks.jsonl, specs_chunks.jsonl)
2. ðŸ“‹ Model laden und vorbereiten (`sentence-transformers`)
3. ðŸ“‹ Batch-Processing aller Chunks:
   - Alle drei JSONL-Files kombinieren
   - Embedding fÃ¼r jedes `document` Field generieren
   - L2-Normalisierung anwenden
4. ðŸ“‹ Speicherung:
   - Embeddings als `.npy` Array
   - Chunks mit IDs als `.jsonl` (fÃ¼r Mapping)
   - Metadata-File mit Model-Info
5. ðŸ“‹ QualitÃ¤tschecks:
   - Keine NaNs/Inf-Werte
   - Korrekte Dimensionen [N, 1024]
   - Semantische PlausibilitÃ¤t (Ã¤hnliche Produkte â†’ Ã¤hnliche Vektoren)

**Expected Output:**
```
2-embedding/
â”œâ”€â”€ embeddings_e5_large.npy          # Vector array [N, 1024]
â”œâ”€â”€ chunks_combined.jsonl            # Alle Chunks mit chunk_id
â””â”€â”€ embedding_metadata.json          # Model name, dimensions, timestamp
```

**NÃ¤chste Schritte nach Fertigstellung:**
- Embedding-QualitÃ¤t visuell prÃ¼fen (t-SNE/UMAP)
- Zur Phase 3 (Indexing) Ã¼bergehen

### ðŸ“‹ TODO: Phase 3 - Vector Database Integration

**Technology:** ChromaDB (einfach, lokal, perfekt fÃ¼r Lernen)

**Setup:**
1. ChromaDB installieren
2. Collection erstellen mit e5-large-Embeddings
3. Alle Chunks mit Metadata laden
4. Index aufbauen

**Schema:**
```python
chunk = {
    "document": "Abmessungen (cm) - AuÃŸenmaÃŸe Breite: 67, Tiefe: 72, HÃ¶he: 132",
    "embedding": [...],  # 1024-dim vector
    "metadata": {
        "chunk_type": "overview|description|specs",
        "product_id": "LABO-288",
        "product_title": "Kirsch LABO-288 PRO-ACTIVE",
        "product_url": "https://...",
        "product_category": "LaborkÃ¼hlschrank",  # Nur bei overview
        "product_manufacturer": "Kirsch",         # Nur bei overview
        "spec_category": "Abmessungen-cm",        # Nur bei specs
        "specs": {"AuÃŸenmaÃŸe_Breite": 67, ...}   # Nur bei specs
    }
}
```

**Output:**
```
3-indexing/
â”œâ”€â”€ chroma_db/                       # ChromaDB Persistenz
â”œâ”€â”€ setup_index.ipynb                # Setup-Code
â””â”€â”€ test_queries.ipynb               # Erste Query-Tests
```

### ðŸ“‹ TODO: Phase 4 - Retrieval Evaluation

**Ziel:** Testen ob Chunking-Strategie und Embeddings funktionieren

**Test-Queries:**
```python
test_queries = {
    'overview': "Welche LaborkÃ¼hlschrÃ¤nke von Liebherr gibt es?",
    'technical_specs': "KÃ¼hlschrank mit 280L und max. 150cm HÃ¶he",
    'features': "Wie funktioniert die TemperaturÃ¼berwachung?",
    'hybrid': "Energieeffizienter MedikamentenkÃ¼hlschrank mit SmartMonitoring"
}
```

**Evaluation-Metriken:**
- **Precision@K** - Wie viele der Top-K sind relevant?
- **Recall@K** - Wie viele relevante Docs wurden gefunden?
- **MRR** - Position des ersten relevanten Dokuments
- **NDCG** - Ranking-QualitÃ¤t

**Success Criteria:**
- Precision@3: >80%
- Recall@5: >70%
- MRR: >0.8
- Overview-Chunks ranken hoch bei Produkt-Queries
- Spec-Chunks ranken hoch bei technischen Multi-Kriterien-Queries
- Description-Chunks ranken hoch bei Feature-Queries

**Falls schlecht:** â†’ Phase 5 (Model Evaluation)

### ðŸ“‹ TODO: Phase 5 - Model Evaluation (Optional)

**Wann:** Nur falls Phase 4 (Retrieval Evaluation) schlechte Ergebnisse zeigt

**Ziel:** Besseres Embedding-Model fÃ¼r deutsche medizinische Fachsprache finden

**Models to Test:**
- `deepset/gbert-large` - Deutsche BERT-Variante
- `GerMedBERT/medbert-512` - Medizin-spezifisch
- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
- Weitere nach Bedarf

**Evaluation-Methoden:**
1. **Semantic Similarity Tests** - Fachbegriffs-Paare (z.B. "MedikamentenkÃ¼hlschrank" â†” "Pharmazeutische Lagerung")
2. **Retrieval-Tests** - Dieselben Test-Queries aus Phase 4
3. **Performance-Metrics** - Embedding-Speed, Memory-Usage

**Success Criteria:**
- Bessere Metrics als e5-large in Phase 4
- Gute Differenzierung zwischen technischen Begriffen
- Robustheit gegenÃ¼ber Synonymen
- Performant genug fÃ¼r Produktion (<2s fÃ¼r 2500 Chunks)

**Output:**
- Falls besseres Model gefunden â†’ Phase 2 & 3 wiederholen
- Sonst â†’ Weiter zu Phase 6

### ðŸ“‹ TODO: Phase 6 - Production RAG Pipeline

**Components:**
1. Query Interface (FastAPI oder Streamlit)
2. Query â†’ Embedding
3. Vector Search (ChromaDB)
4. Context Assembly (Top-K Chunks + Metadata)
5. LLM Integration (Claude/GPT/Mistral)
6. Response Generation
7. Caching + Logging

**Advanced Features (spÃ¤ter):**
- Hybrid Search: Semantic + Structured Filtering
- Multi-Stage Retrieval: Overview â†’ Details
- Metadata-basierte Pre-/Post-Filtering
- Re-Ranking

---

## Neue Chunk-Schema Dokumentation

### Overview-Chunk
```json
{
  "document": "Der Kirsch LABO-288 ist ein LaborkÃ¼hlschrank mit 280L Volumen...",
  "metadata": {
    "chunk_type": "overview",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://...",
    "product_category": "LaborkÃ¼hlschrank",
    "product_manufacturer": "Kirsch"
  }
}
```

### Description-Chunk
```json
{
  "document": "Die elektronische Temperatursteuerung regelt die Temperatur...",
  "metadata": {
    "chunk_type": "description",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://..."
  }
}
```

### Specs-Chunk
```json
{
  "document": "Abmessungen (cm) - AuÃŸenmaÃŸe Breite: 67, Tiefe: 72, HÃ¶he: 132",
  "metadata": {
    "chunk_type": "specs",
    "spec_category": "Abmessungen-cm",
    "product_id": "LABO-288",
    "product_title": "Kirsch LABO-288 PRO-ACTIVE LaborkÃ¼hlschrank",
    "product_url": "https://...",
    "specs": {
      "AuÃŸenmaÃŸe_Breite": 67,
      "AuÃŸenmaÃŸe_Tiefe": 72,
      "AuÃŸenmaÃŸe_HÃ¶he": 132
    }
  }
}
```

---

## Updated File Structure

```
ProduktRAG/
â”œâ”€â”€ _docs/                              # ðŸ“š Documentation
â”‚   â”œâ”€â”€ CHUNKING-DEEP-DIVE.md          # âœ… Umfassender Chunking-Guide
â”‚   â””â”€â”€ EMBEDDING-STRATEGIES.md        # âœ… Embedding-Best-Practices
â”œâ”€â”€ 1-chunking/                        # ðŸ”„ IN PROGRESS - Normalisierung + Chunking
â”‚   â”œâ”€â”€ agent_description.md           # âœ… Description-Agent Prompt
â”‚   â”œâ”€â”€ agent_specs.md                 # âœ… Specs-Agent Prompt
â”‚   â”œâ”€â”€ agent_summary.md               # âœ… Summary-Agent Prompt
â”‚   â”œâ”€â”€ 1-cleanup.ipynb                # ðŸ”„ LLM-Pipeline Implementation
â”‚   â”œâ”€â”€ products_raw.json              # âœ… Raw data (152 Produkte)
â”‚   â”œâ”€â”€ summs_chunks.jsonl             # ðŸ“‹ TODO - Overview chunks
â”‚   â”œâ”€â”€ descs_chunks.jsonl             # ðŸ“‹ TODO - Description chunks
â”‚   â””â”€â”€ specs_chunks.jsonl             # ðŸ“‹ TODO - Specs chunks
â”œâ”€â”€ 2-embedding/                       # ðŸ“‹ TODO - Embedding generation
â”‚   â”œâ”€â”€ generate_embeddings.ipynb      # Embedding-Pipeline
â”‚   â”œâ”€â”€ embeddings_e5_large.npy        # Vector-Ausgabe
â”‚   â”œâ”€â”€ chunks_combined.jsonl          # Alle Chunks kombiniert
â”‚   â””â”€â”€ embedding_metadata.json        # Model-Info
â”œâ”€â”€ 3-indexing/                        # ðŸ“‹ TODO - ChromaDB setup
â”‚   â”œâ”€â”€ chroma_db/                     # ChromaDB Persistenz
â”‚   â”œâ”€â”€ setup_index.ipynb              # Index erstellen
â”‚   â””â”€â”€ test_queries.ipynb             # Erste Tests
â”œâ”€â”€ 4-eval-retrieval/                  # ðŸ“‹ TODO - Retrieval evaluation
â”‚   â”œâ”€â”€ eval_retrieval.ipynb           # Metrics berechnen
â”‚   â””â”€â”€ test_queries.json              # Test-Query-Set
â”œâ”€â”€ 5-eval-model/                      # ðŸ“‹ TODO OPTIONAL - Model comparison
â”‚   â”œâ”€â”€ compare_models.ipynb           # Model-Vergleich
â”‚   â””â”€â”€ results.json                   # Evaluation-Results
â”œâ”€â”€ 6-production/                      # ðŸ“‹ TODO - Production RAG
â”‚   â”œâ”€â”€ app.py                         # FastAPI/Streamlit
â”‚   â”œâ”€â”€ rag_pipeline.py                # RAG-Logik
â”‚   â””â”€â”€ config.yaml                    # Konfiguration
â”œâ”€â”€ ROADMAP.md                         # âœ… This file
â””â”€â”€ requirements.txt                   # âœ… Dependencies
```

---

## Lessons Learned (aus Version 1)

### Was schiefging:
- **Chunking zu frÃ¼h:** Ohne Datenbereinigung â†’ viele schlechte Chunks
- **Kein LLM fÃ¼r Normalisierung:** Manuelle Regex-Spielchen ineffektiv
- **Zu granulare Specs:** Jede Spec einzeln â†’ schlechtes Multi-Kriterien-Matching
- **Keine Hierarchie:** Alle Chunks gleich behandelt

### Was wir jetzt besser machen:
- **LLM-First Approach:** Lass das LLM die Arbeit machen
- **Hierarchische Chunks:** Overview, Details, Specs fÃ¼r verschiedene Query-Typen
- **Strukturierte Normalisierung:** Einheiten vereinheitlicht, gruppiert
- **Quality First:** Lieber 2500 gute Chunks als 4618 mit Noise

### NÃ¤chste Schritte nach Chunking:
1. **Embedding** - Mit multilingual-e5-large starten
2. **Indexing** - ChromaDB aufsetzen
3. **Retrieval Evaluation** - Testen ob's funktioniert
4. **Model Evaluation** - Nur falls nÃ¶tig, andere Models probieren
5. **Production** - RAG-Pipeline bauen

---

## Performance Targets

### Data Quality (Phase 1):
- âœ… Alle Chunks >10 Zeichen
- âœ… Keine Marketing-Floskeln in Descriptions
- âœ… Konsistente Einheiten (cm, kg, l)
- âœ… Strukturierte Gruppierung

### Embedding Performance (Phase 2):
- Embedding-Zeit fÃ¼r 2500 Chunks: <2min
- Embedding-Dimensionen: 1024
- Keine NaNs oder Inf-Werte

### Retrieval Quality (Phase 4):
- Precision@3: >80%
- Recall@5: >70%
- MRR: >0.8
- NDCG: >0.75

### Production Performance (Phase 6):
- Query Latency: <3s (Embedding + Retrieval + Generation)
- Memory Usage: <4GB
- Scalability: 100+ concurrent queries

---

## Documentation

Siehe `_docs/` Ordner fÃ¼r:
- **CHUNKING-DEEP-DIVE.md** - Warum, Was, Wie? (Universell einsetzbar)
- **RAG-EMBEDDING-STRATEGIES.md** - Best Practices fÃ¼r Embeddings

---

*Last updated: 2025-10-03*
*Status: Phase 1 (Chunking) âœ… Complete | Phase 2 (Embedding) ðŸŽ¯ Next*

---

## Quick Reference: Pipeline-Ãœberblick

```
Phase 1: Chunking âœ… DONE
   â†“
Phase 2: Embedding ðŸŽ¯ NEXT (multilingual-e5-large)
   â†“
Phase 3: Indexing (ChromaDB)
   â†“
Phase 4: Retrieval Evaluation (testen!)
   â†“
Phase 5: Model Evaluation (nur falls Phase 4 schlecht)
   â†“
Phase 6: Production RAG
```

### Phase 1 Summary (Completed)
- 3 LLM-Agenten fÃ¼r Normalisierung (summs, descs, specs)
- Chunk-Schema mit chunk_id und strukturierter Metadata
- Specs-Agent mit Dual-Input-Support (Array + Text)
- Bereinigtes Dataset ohne leere Produkte
- Output: 3x JSONL-Files (summs, descs, specs)
