{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435c8504",
   "metadata": {},
   "source": [
    "# Normalization pipeline\n",
    "\n",
    "Die Quelldaten sind aus meinem ersten RAG-Projekt und wurden mit fruendlicher Genehmigung von einer Webseite gescrappt.\n",
    "\n",
    "**Was zu tun ist:**\n",
    "- Daten von Marketingaussagen und Firmengeschichte befreien\n",
    "- Die Specs normalisieren\n",
    "\n",
    "Das meiste werde ich mit einem LLM machen. Die Ausgangsdaten werden stichprobenartig kontrolliert. Ziel ist ein funktionierendes und korrektes retrival, welches nicht zwingend mit der realen Welt (korrekte technische Daten z.B.) übereinstimmen muss. In einem Realworldprojekt ist das selbstverständlich etwas anderes.\n",
    "\n",
    "Wärend der Bereinigung werden die Daten in Specs und Descriptions aufgeteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from mistralai import Mistral\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "df = pd.read_json('./products_raw.json')\n",
    "\n",
    "api_key = os.getenv('MISTRAL_API_KEY')\n",
    "model = 'mistral-large-latest'\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "descs_promt = open('./description_agent.md', 'r').read()\n",
    "specs_promt = open('./specs_agent.md', 'r').read()\n",
    "summs_promt = open('./summary_agent.md').read()\n",
    "\n",
    "descs_chunks = []\n",
    "specs_chunks = []\n",
    "summs_chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80b7dc",
   "metadata": {},
   "source": [
    "## Descriptions\n",
    "\n",
    "Die Beschreibungen werden über ein LLM bereinigt. Der Systempromt liegt als md-File vor. \n",
    "\n",
    "### Funktionen\n",
    "\n",
    "- **agent_request**: Allgemeine Retrivalfunktion \n",
    "- **format_spec**: Bereitet Embeddingdocs für Specs auf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "514a06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_request(promt, content):\n",
    "    response = client.chat.complete(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': promt\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': content,\n",
    "            }\n",
    "        ],\n",
    "        response_format = {\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "def format_spec(group_name, specs_list):\n",
    "    if not specs_list:\n",
    "        return ''\n",
    "    \n",
    "    if '-' in group_name:\n",
    "        label, unit = group_name.rsplit('-', 1)\n",
    "        header = f\"{label} ({unit})\"\n",
    "    else:\n",
    "        header = group_name\n",
    "    \n",
    "    spec_parts = []\n",
    "\n",
    "    for key, value in specs_list.items():\n",
    "        readable_key = key.replace('_', ' ')\n",
    "        spec_parts.append(f\"{readable_key}: {value}\")\n",
    "\n",
    "    return f\"{header} - {', '.join(spec_parts)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8a38d",
   "metadata": {},
   "source": [
    "# Erstellung der Chunks\n",
    "\n",
    "Die Daten werden an ein LLM geliefert und wir erhalten nur die erzeugen Artefarkte:\n",
    "\n",
    "```json\n",
    "# Specs\n",
    "{\n",
    "  \"Abmessungen-cm\": {\n",
    "    \"Außenmaße_Breite\": 67,\n",
    "    \"Außenmaße_Tiefe\": 72,\n",
    "    \"Außenmaße_Höhe\": 132\n",
    "  },\n",
    "  \"Gewicht-kg\": {\n",
    "    \"netto\": 71,\n",
    "    \"brutto\": 83\n",
    "  },\n",
    "  \"Volumen-l\": {\n",
    "    \"Kühlinhalt\": 280\n",
    "  },\n",
    "  \"Temperatur-celsius\": {},\n",
    "  \"Leistung-watt\": {},\n",
    "  \"Energieverbrauch-kwh\": {},\n",
    "  \"Elektrisch\": {},\n",
    "  \"Ausstattung\": {},\n",
    "  \"Sonstiges\": {}\n",
    "}\n",
    "\n",
    "# Beschreibung\n",
    "[\n",
    "  \"Optimierter Absatz zu einem Aspekt des Produkts.\",\n",
    "  \"Optimierter Absatz zu einem weiteren Aspekt des Produkts.\",\n",
    "  \"Optimierter Absatz zu noch einem Aspekt des Produkts.\"\n",
    "]\n",
    "\n",
    "# Summary\n",
    "{\n",
    "  \"summary\": \"Der Kirsch LABO-288 PRO-ACTIVE ist ein Laborkühlschrank mit 280 Litern Nutzvolumen und Temperaturregelung von 0-15°C. Mit statischer Belüftung, automatischer Abtauung und DIN 13221 Konformität ist er ideal für die sichere Lagerung in Labor und Medizin.\",\n",
    "  \"category\": \"Laborkühlschrank\",\n",
    "  \"manufacturer\": \"Kirsch\"\n",
    "}\n",
    "```\n",
    "\n",
    "Diese werden in des Schema der Chunks integriert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0995ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 50/152 [14:11<20:42, 12.18s/it]"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "\n",
    "    summs_response = agent_request(summs_promt, json.dumps(row.to_dict()))\n",
    "    # print(summs_response)\n",
    "    # time.sleep(1)\n",
    "    descs_response = agent_request(descs_promt, row['description'])\n",
    "    # print(descs_response)\n",
    "    # time.sleep(1)\n",
    "    specs_response = agent_request(specs_promt, json.dumps(row['specs']))\n",
    "    # print(specs_response)\n",
    "    # time.sleep(1)\n",
    "\n",
    "    summs_result = json.loads(summs_response.choices[0].message.content)                              \n",
    "    descs_result = json.loads(descs_response.choices[0].message.content)\n",
    "    specs_result = json.loads(specs_response.choices[0].message.content)\n",
    "\n",
    "    category = summs_result['category']\n",
    "    manufacturer = summs_result['manufacturer']\n",
    "\n",
    "    summs_chunks.append({\n",
    "        'document': summs_result['summary'],\n",
    "        'metadata': {\n",
    "            'chunk_type': 'description',\n",
    "            'product_id': row['id'],\n",
    "            'product_title': row['title'],\n",
    "            'product_url': row['url'],\n",
    "            'product_category': category,\n",
    "            'product_manufacturer': manufacturer\n",
    "        }\n",
    "    })\n",
    "\n",
    "    for praragraph in descs_result:\n",
    "        descs_chunks.append({\n",
    "            'document': praragraph,\n",
    "            'metadata': {\n",
    "                'chunk_type': 'description',\n",
    "                'product_id': row['id'],\n",
    "                'product_title': row['title'],\n",
    "                'product_url': row['url'],\n",
    "                'product_category': category,\n",
    "                'product_manufacturer': manufacturer\n",
    "            }\n",
    "        })\n",
    "\n",
    "    for group_name, specs_list in specs_result.items():\n",
    "        if specs_list:\n",
    "            specs_chunks.append({\n",
    "                'document': format_spec(group_name, specs_list),\n",
    "                'metadata': {\n",
    "                    'chunk_type': 'description',\n",
    "                    'product_id': row['id'],\n",
    "                    'product_title': row['title'],\n",
    "                    'product_url': row['url'],\n",
    "                    'product_category': category,\n",
    "                    'product_manufacturer': manufacturer\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Testing only\n",
    "    # if index == 4: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93d0f9",
   "metadata": {},
   "source": [
    "## Quality Checks\n",
    "\n",
    "Wir müssen zuletzt noch in die Chunks schauen ob sie die wichtigsten Rahmenbedinungen einhalten. Vor allem die Länge da die meisten Models eine beschränkte Anzahl an Tokes verarbeiten kann und die darf von den Chunks nicht überschritten werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f169d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df, name):\n",
    "\n",
    "    length = df['document'].str.len()\n",
    "\n",
    "    print(f'--- {name} ---')\n",
    "    print(f\"{length.describe()}\")\n",
    "    print(f'TOTAL CHUNKS {len(df)}')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c904f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summs ---\n",
      "count      6.000000\n",
      "mean     352.000000\n",
      "std       63.545259\n",
      "min      249.000000\n",
      "25%      339.500000\n",
      "50%      350.000000\n",
      "75%      374.750000\n",
      "max      444.000000\n",
      "Name: document, dtype: float64\n",
      "TOTAL CHUNKS 6\n",
      "\n",
      "\n",
      "--- Descriptions ---\n",
      "count     39.000000\n",
      "mean     383.076923\n",
      "std      109.593600\n",
      "min      217.000000\n",
      "25%      307.000000\n",
      "50%      364.000000\n",
      "75%      425.500000\n",
      "max      738.000000\n",
      "Name: document, dtype: float64\n",
      "TOTAL CHUNKS 39\n",
      "\n",
      "\n",
      "--- Specs ---\n",
      "count     35.000000\n",
      "mean     152.257143\n",
      "std      198.847169\n",
      "min       29.000000\n",
      "25%       40.500000\n",
      "50%       75.000000\n",
      "75%      148.000000\n",
      "max      839.000000\n",
      "Name: document, dtype: float64\n",
      "TOTAL CHUNKS 35\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats(pd.DataFrame(summs_chunks), 'Summs')\n",
    "stats(pd.DataFrame(descs_chunks), 'Descriptions')\n",
    "stats(pd.DataFrame(specs_chunks), 'Specs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf904fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(summs_chunks).to_json('summs_chunks.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "pd.DataFrame(descs_chunks).to_json('descs_chunks.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "pd.DataFrame(specs_chunks).to_json('specs_chunks.jsonl', orient='records', lines=True, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
