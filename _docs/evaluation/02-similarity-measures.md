# Similarity Measures

**Worum geht's?** Verschiedene Methoden um die √Ñhnlichkeit zwischen Vektoren zu messen.

**Warum wichtig?** Die Wahl der Similarity-Metrik beeinflusst welche Chunks als "√§hnlich" erkannt werden.

---

## 1. Cosine Similarity ‚≠ê

**Definition:** Misst den Winkel zwischen zwei Vektoren (0¬∞ = identisch, 90¬∞ = orthogonal)

**Formel:**
```
cos(Œ∏) = (a ¬∑ b) / (|a| * |b|)
Range: [-1, 1], meist [0, 1] bei Embeddings
```

**Code:**
```python
# sklearn
from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity([query_emb], embeddings)[0]

# sentence-transformers
from sentence_transformers import util
similarity = util.cos_sim(query_emb, embeddings)[0]

# numpy (wenn normalisiert)
similarity = embeddings @ query_emb  # Dot product = Cosine!
```

**Interpretation:**
- `> 0.8`: Sehr √§hnlich
- `0.6 - 0.8`: √Ñhnlich
- `0.4 - 0.6`: Schwach √§hnlich
- `< 0.4`: Un√§hnlich

**Wann nutzen?**
- ‚úÖ Standard f√ºr Semantic Search
- ‚úÖ Unabh√§ngig von Vektor-L√§nge
- ‚úÖ Development/Testing (explizit, klar)
- ‚ùå Etwas langsamer als Dot Product

**Target:** Relevante Chunks sollten > 0.6 Similarity haben

---

## 2. Dot Product

**Definition:** Summe der elementweisen Multiplikation zweier Vektoren

**Formel:**
```
a ¬∑ b = Œ£(a[i] * b[i])
Range: [-‚àû, +‚àû]
Wenn normalisiert: identisch zu Cosine Similarity
```

**Code:**
```python
# numpy
import numpy as np

# Empfohlen: @ operator
scores = embeddings @ query_emb

# Alternative: np.dot
scores = np.dot(embeddings, query_emb)
```

**Interpretation:**
- H√∂herer Score = √§hnlicher
- Nur zuverl√§ssig wenn Embeddings normalisiert sind
- Dann: identisch zu Cosine (aber schneller!)

**Wann nutzen?**
- ‚úÖ **Production RAG** (schnellste Methode)
- ‚úÖ Wenn Embeddings normalisiert sind
- ‚úÖ Gro√üe Datenmengen (Millionen Vektoren)
- ‚ö†Ô∏è Nur mit Normalisierung zuverl√§ssig

**Target:** Normalisierung aktivieren: `model.encode(..., normalize_embeddings=True)`

---

## 3. Euclidean Distance (L2)

**Definition:** Geradlinige Distanz zwischen zwei Punkten im Vektorraum

**Formel:**
```
dist = ‚àö(Œ£(a[i] - b[i])¬≤)
Range: [0, ‚àû], 0 = identisch
```

**Code:**
```python
# sklearn
from sklearn.metrics.pairwise import euclidean_distances
distances = euclidean_distances([query_emb], embeddings)[0]

# numpy
distances = np.linalg.norm(embeddings - query_emb, axis=1)

# Zu Similarity konvertieren
similarities = 1 / (1 + distances)
```

**Interpretation:**
- Kleinere Distance = √§hnlicher
- Misst absolute Position im Raum
- Geometrisch intuitiv (Luftlinie)

**Wann nutzen?**
- ‚úÖ Clustering (K-Means nutzt Euclidean)
- ‚úÖ Visualisierung (geometrisch intuitiv)
- ‚úÖ Wenn Magnitude wichtig ist
- ‚ùå Nicht Standard f√ºr Semantic Search

**Target:** Nur f√ºr spezielle Use Cases, nicht f√ºr regul√§res Retrieval

---

## 4. Manhattan Distance (L1)

**Definition:** Summe der absoluten Differenzen (Taxicab-Metrik)

**Formel:**
```
dist = Œ£|a[i] - b[i]|
Range: [0, ‚àû]
```

**Code:**
```python
# sklearn
from sklearn.metrics.pairwise import manhattan_distances
distances = manhattan_distances([query_emb], embeddings)[0]

# numpy
distances = np.sum(np.abs(embeddings - query_emb), axis=1)
```

**Interpretation:**
- "Stadtstrecke" statt Luftlinie
- Robust gegen Outlier-Dimensionen
- Kleinere Distance = √§hnlicher

**Wann nutzen?**
- ‚úÖ Sparse, hochdimensionale Vektoren
- ‚úÖ Robustness gegen Outliers wichtig
- ‚ùå Sehr selten in RAG verwendet

**Target:** Nische - nur f√ºr spezifische Anforderungen

---

## üìä Vergleich: Wann welche Similarity?

| Methode | Speed | Normalisierung n√∂tig? | Use Case |
|---------|-------|----------------------|----------|
| **Dot Product** | ‚ö°‚ö°‚ö° Fastest | Ja (sonst unzuverl√§ssig) | **Production RAG** (Standard) |
| **Cosine Similarity** | ‚ö°‚ö° Fast | Nein | Development/Testing |
| **Euclidean** | ‚ö° Slower | Nein | Clustering, Visualisierung |
| **Manhattan** | ‚ö° Slower | Nein | Sparse Data, Robustness |

**Empfehlung f√ºr RAG:**
```python
# In Production:
scores = embeddings @ query_emb  # Dot product (schnell)

# Voraussetzung:
model.encode(..., normalize_embeddings=True)
```
